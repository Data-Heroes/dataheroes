{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "262197b7-d321-4351-97d7-9337d6ca4d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "license_email = dbutils.widgets.get(\"license_email\") #email address linked with your dataheroes account\n",
    "tree_path = dbutils.widgets.get(\"tree_path\")# dbfs path to a volume which will store the coreset data and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b913fae3-9e92-4179-ab1b-9ff70867cdef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Binary Classification Example using GBTClassifier with DataHeroes Spark Library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90daf6a5-c39a-4631-b27b-90695e341ada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The Spark MLlib Pipelines API provides a higher-level API built on top of DataFrames for constructing ML pipelines.\n",
    "You can read more about the Pipelines API in the [MLlib programming guide](https://spark.apache.org/docs/latest/ml-guide.html).\n",
    "\n",
    "The **dh_spark** library leverages the [`DataHeroes`](https://dataheroes.ai/) library inside a Spark cluster for building coreset trees in a distributed environment.\n",
    "\n",
    "The following example is derived from the [Databricks Binary Classification Example](https://docs.databricks.com/aws/en/notebooks/source/binary-classification.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the dh_spark Library\n",
    "\n",
    "To use the `dh_spark` library, you can install it in your Databricks cluster in one of the following ways:\n",
    "\n",
    "1. **Cluster Library Tab**: Upload the provided `.whl` file to your cluster via the Libraries tab in the Databricks UI.\n",
    "\n",
    "2. **Magic Command**: Use the `%pip` magic command to install the library directly in your notebook. For example:\n",
    "```\n",
    "%pip install /dbfs/path/to/your/dh_spark_library.whl\\\n",
    "```\n",
    "\n",
    "Make sure to replace `/dbfs/path/to/your/dh_spark_library.whl` with the actual path to the `.whl` file in your Databricks File System (DBFS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d07f0bd0-a5dc-4d3d-9465-7da99a8c9125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dataset Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44682584-05ce-44f2-86bb-dffe8481b404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The Adult dataset is publicly available at the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Adult).\n",
    "This data derives from census data and consists of information about 48,842 individuals and their annual income.\n",
    "You can use this information to predict if an individual earns **<=50K or >50K** a year.\n",
    "The dataset consists of both numeric and categorical variables.\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "- age: continuous\n",
    "- workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked\n",
    "- fnlwgt: continuous\n",
    "- education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc...\n",
    "- education-num: continuous\n",
    "- marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent...\n",
    "- occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners...\n",
    "- relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried\n",
    "- race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black\n",
    "- sex: Female, Male\n",
    "- capital-gain: continuous\n",
    "- capital-loss: continuous\n",
    "- hours-per-week: continuous\n",
    "- native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany...\n",
    "\n",
    "Target/Label: - <=50K, >50K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03112d5e-6a81-40e0-a6aa-789fa010ca6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1464bdd-05d9-4574-a73c-d06f9bc42eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The Adult dataset is available in Databricks datasets. Read in the data using the CSV data source for Spark and rename the columns appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3c5195d-9b30-4f25-aa16-3ac9c07b52d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls databricks-datasets/adult/adult.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee037428-e4d2-47f4-b4e7-e04bcb764478",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs head databricks-datasets/adult/adult.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b45f2567-65cd-4394-a764-ba4f2cb71b42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, StringType, StructField, StructType\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"age\", DoubleType(), False),\n",
    "  StructField(\"workclass\", StringType(), False),\n",
    "  StructField(\"fnlwgt\", DoubleType(), False),\n",
    "  StructField(\"education\", StringType(), False),\n",
    "  StructField(\"education_num\", DoubleType(), False),\n",
    "  StructField(\"marital_status\", StringType(), False),\n",
    "  StructField(\"occupation\", StringType(), False),\n",
    "  StructField(\"relationship\", StringType(), False),\n",
    "  StructField(\"race\", StringType(), False),\n",
    "  StructField(\"sex\", StringType(), False),\n",
    "  StructField(\"capital_gain\", DoubleType(), False),\n",
    "  StructField(\"capital_loss\", DoubleType(), False),\n",
    "  StructField(\"hours_per_week\", DoubleType(), False),\n",
    "  StructField(\"native_country\", StringType(), False),\n",
    "  StructField(\"income\", StringType(), False)\n",
    "])\n",
    "dataset = spark.read.format(\"csv\").schema(schema).load(\"/databricks-datasets/adult/adult.data\")\n",
    "cols = dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f16ba3a-812a-47d5-acf3-5019291e6134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17449588-3309-482d-989a-d579faef8432",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "### Train data is used for building the tree. This will be passed to the coreset tree service in order to build the tree.\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "print(trainingData.count())\n",
    "print(testData.count())\n",
    "num_instances = trainingData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59528e98-956b-44d6-bf0c-15ff1a7188e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Preprocess Data\n",
    "\n",
    "To use algorithms like Gradient Boosting Classifier, you must first convert the categorical variables in the dataset into numeric variables.\n",
    "There are two ways to do this:\n",
    "\n",
    "* Category Indexing\n",
    "\n",
    "  This is basically assigning a numeric value to each category from {0, 1, 2, ...numCategories-1}.\n",
    "  This introduces an implicit ordering among your categories and is more suitable for ordinal variables (e.g., Poor: 0, Average: 1, Good: 2).\n",
    "\n",
    "* One-Hot Encoding\n",
    "\n",
    "  This converts categories into binary vectors with at most one nonzero value (e.g., (Blue: [1, 0]), (Green: [0, 1]), (Red: [0, 0])).\n",
    "\n",
    "[StringIndexer]: http://spark.apache.org/docs/latest/ml-features.html#stringindexer\n",
    "[OneHotEncoderEstimator]: https://spark.apache.org/docs/2.4.5/api/python/pyspark.ml.html?highlight=one%20hot%20encoder#pyspark.ml.feature.OneHotEncoderEstimator\n",
    "[SparseVector]: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.linalg.SparseVector.html#pyspark.ml.linalg.SparseVector\n",
    "[Pipeline]: https://spark.apache.org/docs/latest/ml-pipeline.html#ml-pipelines\n",
    "[OneHotEncoder]: https://spark.apache.org/docs/latest/ml-features.html#onehotencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65bca570-fa33-4744-b7e4-29eeaf0488c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**The dh_spark library already has this preprocessing implemented under the `build_preprocess_from_df` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4342dc8e-0719-49a6-89c2-92f12d9dcadb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dataheroes.utils import activate_account\n",
    "from dh_pyspark.services.coreset.dtc import CoresetTreeServiceDTC\n",
    "\n",
    "# Activate your DataHeroes account\n",
    "activate_account(license_email)\n",
    "\n",
    "# Set the parameters for the coreset tree service\n",
    "data_tuning_params = {\n",
    "    \"coreset_size\":[0.2],\n",
    "}\n",
    "categoricalColumns = [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\"]\n",
    "features = list({\"name\": col, \"categorical\":True} if col in categoricalColumns else {\"name\":col} for col in cols if col != \"income\" and col != \"label\")# filter out income and label as they are converted to label and features\n",
    "label  = \"income\"\n",
    "\n",
    " # Define data parameters\n",
    " # Those params are related to the data and it's preprocessing\n",
    "data_params = {\n",
    "    \"target\": {\"name\": label}, # Ensure the label is unique in the DataFrame   \n",
    "    \"features\": features,\n",
    "    'fill_value_cat': 'NNN',\n",
    "\n",
    "}\n",
    "# Initialize coreset tree service\n",
    "service = CoresetTreeServiceDTC(\n",
    "    spark_session=spark,\n",
    "    data_params=data_params,\n",
    "    chunk_size=int(num_instances//4),\n",
    "    n_instances=num_instances,\n",
    "    data_tuning_params=data_tuning_params,\n",
    "    dhspark_path=tree_path,\n",
    ")\n",
    "\n",
    "\n",
    "input_df = trainingData\n",
    "service.build_preprocess_from_df(spark_session=spark, input_df=input_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c664542-40cf-48fc-8f92-40b24acaf3c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Build the Coreset Tree\n",
    "This step builds the coreset tree following the preprocessing done in the previous step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ed8f2f9-29ac-4a98-86ef-2b8083e98a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "service.build(spark_session=spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Data from the Coreset Tree\n",
    "The coreset tree is returned as a Spark DataFrame, which can be passed to any ML model following the Spark interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5166e839-9d8b-4026-ac08-3679473198cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trainingData = service.get_coreset(spark_session=spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5440730-11c9-4e23-8584-7e9bfe31dbb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fit and Evaluate Models\n",
    "\n",
    "We are using the Gradient Boosting Classifier from MLlib's classification algorithms:\n",
    "  - GBTClassifier (Gradient Boosted Tree Classifier)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9bd8fd7-2904-472d-bd74-020deda61a98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Gradient Boosting Classifier\n",
    "\n",
    "You can read more about [Gradient Boosting Classifier] from the [classification and regression] section of the MLlib Programming Guide.\n",
    "In the Pipelines API, Gradient Boosting Classifier is a powerful ensemble learning method that builds a series of decision trees sequentially, with each tree correcting errors made by the previous ones.\n",
    "\n",
    "[classification and regression]: https://spark.apache.org/docs/latest/ml-classification-regression.html\n",
    "[Gradient Boosting Classifier]: https://spark.apache.org/docs/latest/ml-classification-regression.html#gradient-boosted-tree-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "827dda72-cf41-450f-a6ee-df04690e94cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# Create initial GBTClassifier model\n",
    "# The income label is a categorical variable with two values: <=50K and >50K\n",
    "# dh_pyspark converted this to a numeric index\n",
    "# The column with the preprocessed label is named by appending `index` to the label column name and the features are in a column named \"features\"\n",
    "# This has to be passed to the GBTClassifier\n",
    "gbtc = GBTClassifier(labelCol=f\"{label}_index\", featuresCol=\"features\", maxIter=10, weightCol=\"w\")\n",
    "\n",
    "# Train model with Training Data\n",
    "gbtc = gbtc.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31d04cbc-5046-4c7b-947b-c0584b6b71ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess the test data in the same way the training data was preprocessed\n",
    "testData =service.auto_preprocessing(spark_session=spark, df=testData)\n",
    "\n",
    "# Make predictions on test data using the transform() method.\n",
    "# GBTClassifier.transform() will only use the 'features' column.\n",
    "predictions = gbtc.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8cd5f15-0059-43b0-85c4-8aa4fcb6e717",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# View model's predictions and probabilities of each prediction class\n",
    "# You can select any columns in the above schema to view as well\n",
    "selected = predictions.select(f\"{label}_index\", \"prediction\", \"probability\")\n",
    "display(selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f452d12e-ddb1-42e7-a1b5-bedb70666dd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Use `BinaryClassificationEvaluator` and `MulticlassClassificationEvaluator` to evaluate the model.\n",
    "\n",
    "`BinaryClassificationEvaluator` to evaluate the model's [areaUnderROC] metric.\n",
    "\n",
    "[areaUnderROC]: https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve\n",
    "\n",
    "`MulticlassClassificationEvaluator` to evaluate the model's [F1](https://en.wikipedia.org/wiki/F-score) metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08abb97c-e8c5-4ff9-a9ce-69c65989681d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Evaluate model - AUC\n",
    "auc_evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\", labelCol=f\"{label}_index\")\n",
    "auc = auc_evaluator.evaluate(predictions)\n",
    "print(f\"Area under ROC curve: {auc}\")\n",
    "\n",
    "# Evaluate model - F1 Score\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=f\"{label}_index\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = f1_evaluator.evaluate(predictions)\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3650789502557811,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "gbtc-dataheroes",
   "widgets": {
    "license_email": {
     "currentValue": "aristo@dataheroes.ai",
     "nuid": "318b78d2-f53c-48f6-bc7d-8882d75d3fdc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "License Email",
      "name": "license_email",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": "License Email",
      "name": "license_email",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "tree_path": {
     "currentValue": "dbfs:/Volumes/dataheroes_1509835693157163/workflows/coresets",
     "nuid": "ece4dfc7-d38e-48e3-a385-483a275dddde",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "tree_path",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": "",
      "name": "tree_path",
      "options": {
       "autoCreated": false,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
