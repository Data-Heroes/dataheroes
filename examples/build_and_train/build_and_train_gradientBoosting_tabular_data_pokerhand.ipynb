{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build and Train Decision Trees for Classification Problems\n",
    "## Purpose\n",
    "In this example we will demonstrate how to:\n",
    "   - Build a coreset tree for classification decision trees based on a real-world multiclass dataset.\n",
    "   - Perform coreset tree vs. entire train dataset vs. random selection comparison as follows:\n",
    "      - Retrieve the coreset from the coreset tree using get_coreset, and train an sklearn's GradientBoostingClassifier model on it.\n",
    "      - Build an GradientBoostingClassifier model on the entire train dataset.\n",
    "      - Build an GradientBoostingClassifier model on a randomly-sampled train dataset, where the sample size exactly matches coreset size.\n",
    "      - Build an GradientBoostingClassifier model on a randomly-sampled train dataset, where the sample size is large-enough to strive to match the quality of using the coreset from the coreset tree.\n",
    "      - Compare the quality of all four models built.\n",
    "   - Repeat the comparison sets above, using two additional flavours:\n",
    "      - Utilizing GradientBoostingClassifier sklearn classifier.\n",
    "      - Calling our library's service object's direct fit function (employing GradientBoostingClassifier behind the scenes).\n",
    "   - Employ coreset tree's utility functions for saving, loading and plotting the coreset tree.\n",
    "\n",
    "For the example dataset, we will be using the PokerHand OpenML dataset (https://www.openml.org/search?type=data&status=active&id=155).\n",
    "The data will be split into 'train' and 'test' portions using sklearn's train/test split (80% for the train portion, 20% for the test).\n",
    "The 'train' portion will consist of roughly 663K samples, and the 'test' portion will have around 166K samples.\n",
    "Our comparisons will utilize the balanced accuracy (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html) as the evaluation metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T19:17:02.722151Z",
     "start_time": "2023-06-16T19:17:01.309500Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "\n",
    "\n",
    "from dataheroes import CoresetTreeServiceDTC\n",
    "\n",
    "warnings.simplefilter('ignore', DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Prepare the dataset & the testing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T19:17:11.144712Z",
     "start_time": "2023-06-16T19:17:02.726654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#classes=5, train size=663,360, test size=165,841\n"
     ]
    }
   ],
   "source": [
    "# Load PokerHand dataset as X, y (data, target) from openml.\n",
    "X, y = fetch_openml(\"pokerhand\", return_X_y=True)\n",
    "\n",
    "# Label-Encode the target. We do so because, further down the road, we will use GradientBoostingClassifier for\n",
    "# modeling, which requires consecutive integer labels starting from 0.\n",
    "label_enc = LabelEncoder()\n",
    "y = label_enc.fit_transform(y)\n",
    "\n",
    "# Merge 3 class labels into a single label, to make the data-set have only 5 targets.\n",
    "# (Otherwise, the smaller class is made up of only 2 samples, and the last but one - with only 11.)\n",
    "y[y > 3] = 4\n",
    "\n",
    "# Apply standard scaling on the X.\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split to train and test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=62)\n",
    "\n",
    "# Remember the number of classes (5) required for the upcoming training code.\n",
    "num_classes = len(np.unique(y_train))\n",
    "print(f\"#classes={num_classes}, train size={len(X_train):,}, test size={len(X_test):,}\")\n",
    "\n",
    "# Define the number of estimators for GradientBoostingClassifier.\n",
    "# We set the number of estimators to 50 because GradientBoostingClassifier is a slow learner and for the sake of time we \n",
    "# reduce the number of estimators. \n",
    "# Note that in this case the scores will be lower than if we had used a higher number of estimators.\n",
    "# The higher the number of estimators, the better the results will be (but the longer the time will take).\n",
    "n_estimators = 50\n",
    "\n",
    "# Set the tree access level which we will be working with (default is 0=root node).\n",
    "tree_query_level = 2\n",
    "\n",
    "# Remember the number of samples for the full dataset.\n",
    "n_samples_full = len(y_train)\n",
    "\n",
    "# Define the number of samples for the comparison flavour in which a randomly-selected dataset is striving to resemble coreset dataset's quality.\n",
    "n_samples_rand_large = int(n_samples_full * 0.3)\n",
    "\n",
    "# Define method for summarizing every flavour's experimental results in one neat table.\n",
    "def produce_results(experiment_group : str,\n",
    "                    n_samp_full : int, n_samp_rand_large : int, n_samp_coreset : int,\n",
    "                    full_score : float, rand_large_score : float, rand_csize_score : float, coreset_score : float,\n",
    "                    full_secs : float, rand_large_secs : float, rand_csize_secs : float, coreset_secs : float):\n",
    "\n",
    "    df = pd.DataFrame(data={\n",
    "        ' ': ['Full dataset', 'Random bigger-sized sample', 'Random smaller-sized sample', 'Coreset'],\n",
    "        'Training dataset size': [n_samp_full, n_samp_rand_large, n_samp_coreset, n_samp_coreset],\n",
    "        '% of full dataset': [n_samp_full / n_samp_full, n_samp_rand_large / n_samp_full, n_samp_coreset / n_samp_full, n_samp_coreset / n_samp_full],\n",
    "        'Balanced accuracy score': [full_score, rand_large_score, rand_csize_score, coreset_score],\n",
    "        'Training time (sec)': [full_secs, rand_large_secs, rand_csize_secs, coreset_secs],\n",
    "    })\n",
    "    last_row = pd.IndexSlice[df.index[-1], :]\n",
    "    styles = [dict(selector=\"caption\", props=[(\"text-align\", \"center\"), (\"font-size\", \"120%\"), (\"font-weight\", \"bold\")])]\n",
    "    s  = df.style \\\n",
    "        .set_properties(subset=[' '],**{'text-align':'left'}) \\\n",
    "        .set_properties(subset=['Training dataset size','% of full dataset','Balanced accuracy score','Training time (sec)'],**{'text-align':'right'}) \\\n",
    "        .set_properties(subset=last_row, **{'color':'green', 'font-weight':'bold'}) \\\n",
    "        .set_caption(f\"{experiment_group} Results\") \\\n",
    "        .format({\n",
    "        'Training dataset size': '{:,}',\n",
    "        '% of full dataset': '{:.2%}',\n",
    "        'Balanced accuracy score': '{:.4f}',\n",
    "        'Training time (sec)': '{:.2f}'}) \\\n",
    "        .hide(axis='index') \\\n",
    "        .set_table_styles(styles)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the coreset tree\n",
    "Create a new service object and run `build` directly on the X, y train data.\n",
    "We pass `chunk_size` and `coreset_size` instead of passing `n_instances` to initialize the coreset tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T19:17:26.515993Z",
     "start_time": "2023-06-16T19:17:11.148350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coreset build time (sec): 12.30\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "# Define all columns as categorical, so they would be one-hot encoded by the dataheroes library.\n",
    "data_params = {\n",
    "    'categorical_features': [i for i in range(X_train.shape[1])],\n",
    "    'ohe_min_frequency': 0\n",
    "}\n",
    "# We explicitly set the model to GradientBoostingClassifier, as the default\n",
    "# model class is XGBClassifier in case XGBoost is installed\n",
    "service_obj = CoresetTreeServiceDTC(optimized_for='training',\n",
    "                                    chunk_size=40_000,\n",
    "                                    coreset_size=20_000,\n",
    "                                    data_params=data_params,\n",
    "                                    model_cls=GradientBoostingClassifier,\n",
    "                                   )\n",
    "service_obj.build(X_train, y_train,copy=True)\n",
    "coreset_build_secs = time() - t\n",
    "print(f\"Coreset build time (sec): {coreset_build_secs:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tree-based coreset performance comparison: GradientBoostingClassifier modeling approach\n",
    "\n",
    "### 3.1 Get a coreset from the tree & train a model based on it\n",
    "\n",
    "Get the coreset from the tree using the ‘auto’ preprocessing_stage, which returns the data after applying auto-preprocessing, including one-hot-encoding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T19:19:07.856603Z",
     "start_time": "2023-06-16T19:17:26.518682Z"
    }
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "coreset = service_obj.get_coreset(level=tree_query_level, preprocessing_stage='auto')\n",
    "indices_coreset_, X_coreset, y_coreset = coreset['data']\n",
    "w_coreset = coreset['w']\n",
    "\n",
    "# Train an GradientBoostingClassifier model on the coreset.\n",
    "GradientBoostingClassifier_coreset_model = GradientBoostingClassifier(\n",
    "                                            n_estimators=n_estimators)\n",
    "GradientBoostingClassifier_coreset_model.fit(X_coreset, y_coreset, sample_weight=w_coreset)\n",
    "n_samples_coreset = len(y_coreset)\n",
    "GradientBoostingClassifier_coreset_secs = time() - t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Train a model on the entire dataset (for comparison)\n",
    "We use the same (entire) training part of the dataset that was used for building the coreset tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode all columns (not only the categorical ones).\n",
    "# We may, in fact, encode only categorical columns - but we do the inclusive encoding\n",
    "# here, since even the numeric features have small number of distinct values.\n",
    "one_hot = OneHotEncoder(sparse=False)\n",
    "one_hot.fit(X_train)\n",
    "X_train = one_hot.transform(X_train)\n",
    "X_train = X_train.astype(np.int8)\n",
    "# save original X_test\n",
    "X_test_original = X_test.copy()\n",
    "# same preparation for X_test\n",
    "X_test = one_hot.transform(X_test)\n",
    "X_test = X_test.astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T19:22:36.184427Z",
     "start_time": "2023-06-16T19:19:07.864996Z"
    }
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "# Train an GradientBoostingClassifier model on the entire data-set.\n",
    "GradientBoostingClassifier_full_model = GradientBoostingClassifier(\n",
    "                                         n_estimators=n_estimators)\n",
    "GradientBoostingClassifier_full_model.fit(X_train, y_train)\n",
    "GradientBoostingClassifier_full_secs = time() - t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Train models on samples selected randomly from the training set\n",
    "\n",
    "### 3.3.1. Produce a randomly-selected sample matching the coreset's size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T19:22:50.571392Z",
     "start_time": "2023-06-16T19:22:36.181478Z"
    }
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "# Train an GradientBoostingClassifier model on the random data-set -\n",
    "# (1) Create the model, (2) randomly sample the exact size as the coreset size, and (3) train the model.\n",
    "GradientBoostingClassifier_rand_csize_model = GradientBoostingClassifier(\n",
    "                                               n_estimators=n_estimators)\n",
    "GradientBoostingClassifier_rand_csize_idxs = np.random.choice(n_samples_full, n_samples_coreset, replace=False)\n",
    "GradientBoostingClassifier_rand_csize_model.fit(X_train[GradientBoostingClassifier_rand_csize_idxs, :], y_train[GradientBoostingClassifier_rand_csize_idxs])\n",
    "GradientBoostingClassifier_rand_csize_secs = time() - t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. Produce a randomly-selected sample striving to match coreset's quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T19:24:08.929487Z",
     "start_time": "2023-06-16T19:22:50.568727Z"
    }
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "# Train an GradientBoostingClassifier model on the random data-set -\n",
    "# (1) Create the model, (2) randomly sample size larger than the coreset size, and (3) train the model.\n",
    "GradientBoostingClassifier_rand_large_model = GradientBoostingClassifier(\n",
    "                                               n_estimators=n_estimators)\n",
    "GradientBoostingClassifier_rand_large_idxs = np.random.choice(n_samples_full, n_samples_rand_large, replace=False)\n",
    "GradientBoostingClassifier_rand_large_model.fit(X_train[GradientBoostingClassifier_rand_large_idxs, :], y_train[GradientBoostingClassifier_rand_large_idxs])\n",
    "GradientBoostingClassifier_rand_large_secs = time() - t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Compare models quality\n",
    "Test coreset, random samples and full models on a test dataset, and compare their evaluation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T19:24:09.960757Z",
     "start_time": "2023-06-16T19:24:08.933265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e52b4 caption {\n",
       "  text-align: center;\n",
       "  font-size: 120%;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_e52b4_row0_col0, #T_e52b4_row1_col0, #T_e52b4_row2_col0 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_e52b4_row0_col1, #T_e52b4_row0_col2, #T_e52b4_row0_col3, #T_e52b4_row0_col4, #T_e52b4_row1_col1, #T_e52b4_row1_col2, #T_e52b4_row1_col3, #T_e52b4_row1_col4, #T_e52b4_row2_col1, #T_e52b4_row2_col2, #T_e52b4_row2_col3, #T_e52b4_row2_col4 {\n",
       "  text-align: right;\n",
       "}\n",
       "#T_e52b4_row3_col0 {\n",
       "  text-align: left;\n",
       "  color: green;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_e52b4_row3_col1, #T_e52b4_row3_col2, #T_e52b4_row3_col3, #T_e52b4_row3_col4 {\n",
       "  text-align: right;\n",
       "  color: green;\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e52b4\">\n",
       "  <caption>GradientBoostingClassifier Results</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_e52b4_level0_col0\" class=\"col_heading level0 col0\" > </th>\n",
       "      <th id=\"T_e52b4_level0_col1\" class=\"col_heading level0 col1\" >Training dataset size</th>\n",
       "      <th id=\"T_e52b4_level0_col2\" class=\"col_heading level0 col2\" >% of full dataset</th>\n",
       "      <th id=\"T_e52b4_level0_col3\" class=\"col_heading level0 col3\" >Balanced accuracy score</th>\n",
       "      <th id=\"T_e52b4_level0_col4\" class=\"col_heading level0 col4\" >Training time (sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_e52b4_row0_col0\" class=\"data row0 col0\" >Full dataset</td>\n",
       "      <td id=\"T_e52b4_row0_col1\" class=\"data row0 col1\" >663,360</td>\n",
       "      <td id=\"T_e52b4_row0_col2\" class=\"data row0 col2\" >100.00%</td>\n",
       "      <td id=\"T_e52b4_row0_col3\" class=\"data row0 col3\" >0.4959</td>\n",
       "      <td id=\"T_e52b4_row0_col4\" class=\"data row0 col4\" >577.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e52b4_row1_col0\" class=\"data row1 col0\" >Random bigger-sized sample</td>\n",
       "      <td id=\"T_e52b4_row1_col1\" class=\"data row1 col1\" >199,008</td>\n",
       "      <td id=\"T_e52b4_row1_col2\" class=\"data row1 col2\" >30.00%</td>\n",
       "      <td id=\"T_e52b4_row1_col3\" class=\"data row1 col3\" >0.4881</td>\n",
       "      <td id=\"T_e52b4_row1_col4\" class=\"data row1 col4\" >153.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e52b4_row2_col0\" class=\"data row2 col0\" >Random smaller-sized sample</td>\n",
       "      <td id=\"T_e52b4_row2_col1\" class=\"data row2 col1\" >43,968</td>\n",
       "      <td id=\"T_e52b4_row2_col2\" class=\"data row2 col2\" >6.63%</td>\n",
       "      <td id=\"T_e52b4_row2_col3\" class=\"data row2 col3\" >0.4735</td>\n",
       "      <td id=\"T_e52b4_row2_col4\" class=\"data row2 col4\" >16.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e52b4_row3_col0\" class=\"data row3 col0\" >Coreset</td>\n",
       "      <td id=\"T_e52b4_row3_col1\" class=\"data row3 col1\" >43,968</td>\n",
       "      <td id=\"T_e52b4_row3_col2\" class=\"data row3 col2\" >6.63%</td>\n",
       "      <td id=\"T_e52b4_row3_col3\" class=\"data row3 col3\" >0.5868</td>\n",
       "      <td id=\"T_e52b4_row3_col4\" class=\"data row3 col4\" >17.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x21100332340>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate models.\n",
    "GradientBoostingClassifier_full_score = balanced_accuracy_score(y_test, GradientBoostingClassifier_full_model.predict(X_test))\n",
    "GradientBoostingClassifier_rand_large_score = balanced_accuracy_score(y_test, GradientBoostingClassifier_rand_large_model.predict(X_test))\n",
    "GradientBoostingClassifier_rand_csize_score = balanced_accuracy_score(y_test, GradientBoostingClassifier_rand_csize_model.predict(X_test))\n",
    "# We call auto_preprocessing on the test data, to prepare it in the same way\n",
    "# the train data was processed and was used to build the model.\n",
    "GradientBoostingClassifier_coreset_score = balanced_accuracy_score(y_test, GradientBoostingClassifier_coreset_model.predict(service_obj.auto_preprocessing(X=X_test_original, copy=True)['data']))\n",
    "produce_results(\"GradientBoostingClassifier\",\n",
    "                n_samples_full, n_samples_rand_large, n_samples_coreset,\n",
    "                GradientBoostingClassifier_full_score, GradientBoostingClassifier_rand_large_score, GradientBoostingClassifier_rand_csize_score, GradientBoostingClassifier_coreset_score,\n",
    "                GradientBoostingClassifier_full_secs, GradientBoostingClassifier_rand_large_secs, GradientBoostingClassifier_rand_csize_secs, GradientBoostingClassifier_coreset_secs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tree-based coreset performance comparison: coreset tree's default fit modeling approach\n",
    "\n",
    "### 4.1 Train a model directly on the coreset tree service object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GradientBoostingClassifier model. To use a different model, please set the model parameter. E.g.: service_obj.fit(model=LGBMClassifier(**params), **other_params) or set the model_cls during init or use the set_model_cls function\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "# Ignore GradientBoostingClassifier's warning about the requirement to pass the 'sample_weight' \n",
    "# as a keyword arg and not as a positional arg.\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Please note the direct usage of \"service_obj.fit()\".\n",
    "# Provide the same parameters to the fit method as you'd provide for the model generation and \n",
    "# for tree access (required tree level).\n",
    "service_obj.fit(level=tree_query_level,\n",
    "                n_estimators=n_estimators\n",
    "               )\n",
    "service_obj_n_samples_coreset = len(service_obj.get_coreset(level=tree_query_level)['w'])\n",
    "service_obj_coreset_secs = time() - t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Train a model on the entire dataset (for comparison)\n",
    "We use the same (entire) training part of the dataset that was used for building the coreset tree.\n",
    "(Please note that for the sake of time saving, we reuse the same exact model generated in the step 3.2. above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just assign the same model and execution time, and do not call `fit`, since the model has already been trained.\n",
    "service_obj_full_model = GradientBoostingClassifier_full_model\n",
    "service_obj_full_secs = GradientBoostingClassifier_full_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Train models on samples selected randomly from the training set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just assign the same model and execution time, and do not call `fit`, since the model has already been trained.\n",
    "service_obj_rand_csize_model = GradientBoostingClassifier_rand_csize_model\n",
    "service_obj_rand_csize_secs = GradientBoostingClassifier_rand_csize_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. Produce a randomly-selected sample striving to match coreset's quality\n",
    "(Please note that for the sake of time saving, we reuse the same exact model generated in the step 3.3.2. above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just assign the same model and execution time, and do not call `fit`, since the model has already been trained.\n",
    "service_obj_rand_large_model = GradientBoostingClassifier_rand_large_model\n",
    "service_obj_rand_large_secs = GradientBoostingClassifier_rand_large_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Compare models quality\n",
    "Test coreset, random samples and full models on a test dataset, and compare their evaluation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_88c72 caption {\n",
       "  text-align: center;\n",
       "  font-size: 120%;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_88c72_row0_col0, #T_88c72_row1_col0, #T_88c72_row2_col0 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_88c72_row0_col1, #T_88c72_row0_col2, #T_88c72_row0_col3, #T_88c72_row0_col4, #T_88c72_row1_col1, #T_88c72_row1_col2, #T_88c72_row1_col3, #T_88c72_row1_col4, #T_88c72_row2_col1, #T_88c72_row2_col2, #T_88c72_row2_col3, #T_88c72_row2_col4 {\n",
       "  text-align: right;\n",
       "}\n",
       "#T_88c72_row3_col0 {\n",
       "  text-align: left;\n",
       "  color: green;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_88c72_row3_col1, #T_88c72_row3_col2, #T_88c72_row3_col3, #T_88c72_row3_col4 {\n",
       "  text-align: right;\n",
       "  color: green;\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_88c72\">\n",
       "  <caption>Service Object (Default) Results</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_88c72_level0_col0\" class=\"col_heading level0 col0\" > </th>\n",
       "      <th id=\"T_88c72_level0_col1\" class=\"col_heading level0 col1\" >Training dataset size</th>\n",
       "      <th id=\"T_88c72_level0_col2\" class=\"col_heading level0 col2\" >% of full dataset</th>\n",
       "      <th id=\"T_88c72_level0_col3\" class=\"col_heading level0 col3\" >Balanced accuracy score</th>\n",
       "      <th id=\"T_88c72_level0_col4\" class=\"col_heading level0 col4\" >Training time (sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_88c72_row0_col0\" class=\"data row0 col0\" >Full dataset</td>\n",
       "      <td id=\"T_88c72_row0_col1\" class=\"data row0 col1\" >663,360</td>\n",
       "      <td id=\"T_88c72_row0_col2\" class=\"data row0 col2\" >100.00%</td>\n",
       "      <td id=\"T_88c72_row0_col3\" class=\"data row0 col3\" >0.4959</td>\n",
       "      <td id=\"T_88c72_row0_col4\" class=\"data row0 col4\" >577.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_88c72_row1_col0\" class=\"data row1 col0\" >Random bigger-sized sample</td>\n",
       "      <td id=\"T_88c72_row1_col1\" class=\"data row1 col1\" >199,008</td>\n",
       "      <td id=\"T_88c72_row1_col2\" class=\"data row1 col2\" >30.00%</td>\n",
       "      <td id=\"T_88c72_row1_col3\" class=\"data row1 col3\" >0.4881</td>\n",
       "      <td id=\"T_88c72_row1_col4\" class=\"data row1 col4\" >153.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_88c72_row2_col0\" class=\"data row2 col0\" >Random smaller-sized sample</td>\n",
       "      <td id=\"T_88c72_row2_col1\" class=\"data row2 col1\" >43,968</td>\n",
       "      <td id=\"T_88c72_row2_col2\" class=\"data row2 col2\" >6.63%</td>\n",
       "      <td id=\"T_88c72_row2_col3\" class=\"data row2 col3\" >0.4735</td>\n",
       "      <td id=\"T_88c72_row2_col4\" class=\"data row2 col4\" >16.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_88c72_row3_col0\" class=\"data row3 col0\" >Coreset</td>\n",
       "      <td id=\"T_88c72_row3_col1\" class=\"data row3 col1\" >43,968</td>\n",
       "      <td id=\"T_88c72_row3_col2\" class=\"data row3 col2\" >6.63%</td>\n",
       "      <td id=\"T_88c72_row3_col3\" class=\"data row3 col3\" >0.5868</td>\n",
       "      <td id=\"T_88c72_row3_col4\" class=\"data row3 col4\" >17.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2116ad640d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate models. Please note the direct usage of \"service_obj.predict()\".\n",
    "service_obj_full_score = balanced_accuracy_score(y_test, service_obj_full_model.predict(X_test))\n",
    "service_obj_rand_large_score = balanced_accuracy_score(y_test, service_obj_rand_large_model.predict(X_test))\n",
    "service_obj_rand_csize_score = balanced_accuracy_score(y_test, service_obj_rand_csize_model.predict(X_test))\n",
    "# pass original data to predict, due to its build-in preprocessing\n",
    "service_obj_coreset_score = balanced_accuracy_score(y_test, service_obj.predict(X_test_original,copy=True))\n",
    "produce_results(\"Service Object (Default)\",\n",
    "                n_samples_full, n_samples_rand_large, service_obj_n_samples_coreset,\n",
    "                service_obj_full_score, service_obj_rand_large_score, service_obj_rand_csize_score, service_obj_coreset_score,\n",
    "                service_obj_full_secs, service_obj_rand_large_secs, service_obj_rand_csize_secs, service_obj_coreset_secs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tree utilities\n",
    "\n",
    "### 5.1. Save the tree for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path('output')\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "save_tree_name = 'train_coreset_tree'\n",
    "service_obj.save(out_dir, save_tree_name, override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Load the saved tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_obj = CoresetTreeServiceDTC.load(out_dir, save_tree_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Plot the tree\n",
    "Plot the tree to show the loading from files was properly done and to visualize the way the data is structured inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path('output')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "service_obj.plot(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Save coreset for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_coreset_path = os.path.join(output_dir, \"final_coreset\")\n",
    "service_obj.save_coreset(final_coreset_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
