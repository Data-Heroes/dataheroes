{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f7d111a-a562-4ce5-aa85-923eab8dde66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dataheroes import CoresetTreeServiceDTC\n",
    "\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3217e4af-9f90-4eeb-8876-4cfeca5710c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_s3_client():\n",
    "    # replace with your credentials\n",
    "    aws_access_key_id = 'your-access-key'\n",
    "    saws_secret_access_key='your-secret-key'\n",
    "    return boto3.client('s3', aws_access_key_id=aws_access_key_id , aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "def upload_folder_to_s3(folder_path, bucket_name, s3_prefix=\"\"):\n",
    "    s3_client = get_s3_client()\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            local_path = os.path.join(root, file_name)\n",
    "            relative_path = os.path.relpath(local_path, folder_path)\n",
    "            s3_path = os.path.join(s3_prefix, relative_path).replace(\"\\\\\", \"/\")  # Ensure correct path format for S3\n",
    "            try:\n",
    "                s3_client.upload_file(local_path, bucket_name, s3_path)\n",
    "                print(f'Successfully uploaded {local_path} to s3://{bucket_name}/{s3_path}')\n",
    "            except Exception as e:\n",
    "                print(f'Failed to upload {local_path} to s3://{bucket_name}/{s3_path}: {e}')\n",
    "\n",
    "def download_folder_from_s3(bucket_name, s3_prefix, local_dir):\n",
    "    s3_client = get_s3_client()\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket=bucket_name, Prefix=s3_prefix)\n",
    "    for page in pages:\n",
    "        if 'Contents' in page:\n",
    "            for obj in page['Contents']:\n",
    "                s3_key = obj['Key']\n",
    "                relative_path = os.path.relpath(s3_key, s3_prefix)\n",
    "                local_path = os.path.join(local_dir, relative_path)\n",
    "                os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "                try:\n",
    "                    s3_client.download_file(bucket_name, s3_key, local_path)\n",
    "                    print(f'Successfully downloaded {s3_key} to {local_path}')\n",
    "                except Exception as e:\n",
    "                    print(f'Failed to download {s3_key}: {e}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5093fba6-f008-47e4-839d-dae050874768",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create dataset as pandas DataFrame\n",
    "num_rows = 10_000\n",
    "num_float_cols = 10\n",
    "float_data = np.random.rand(num_rows, num_float_cols)\n",
    "target_data = np.random.randint(0, 2, num_rows)\n",
    "df = pd.DataFrame(float_data, columns=[f'feature_{i+1}' for i in range(num_float_cols)])\n",
    "df['target'] = target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "510320ca-7bff-472b-adf5-e18a445e4057",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "service_obj = CoresetTreeServiceDTC(\n",
    "    data_params={'target': {'name': 'target'}},\n",
    "    coreset_size=1_000,\n",
    "    chunk_size=1_000,\n",
    "    optimized_for='training',\n",
    "    working_directory='/local_disk0/'\n",
    ")\n",
    "# build Coreset Tree \n",
    "service_obj.build_from_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5ccd93e-6bea-4caa-8b5e-9e2ffbc938e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save tree locally and upload to s3\n",
    "service_obj_save_path = f'/local_disk0/service_tree_{datetime.now().strftime(\"%d%m%Y_%H%M%S\")}'\n",
    "bucket_name = \"bucket_name\"\n",
    "s3_path = service_obj_save_path.replace('/local_disk0/', '')\n",
    "\n",
    "service_obj.save(service_obj_save_path)\n",
    "upload_folder_to_s3(service_obj_save_path, bucket_name, s3_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3acfc71-01a2-4b7b-ab52-18ad2a0adeb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# download tree from s3 to new destination\n",
    "service_obj_download_save_path = f'/local_disk0/download_service_tree_{datetime.now().strftime(\"%d%m%Y_%H%M%S\")}'\n",
    "download_folder_from_s3(bucket_name, s3_path, service_obj_download_save_path)\n",
    "# load tree to new service object\n",
    "service_obj_new = CoresetTreeServiceDTC.load(service_obj_download_save_path)\n",
    "# checking results\n",
    "service_obj_new.print()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "s3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
