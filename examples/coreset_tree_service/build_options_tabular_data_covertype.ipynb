{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "In this example we demonstrate how to:\n",
    "   - Build a coreset tree from file(s):\n",
    "       - Build from single files\n",
    "       - Build from list of files\n",
    "       - Build from all files in folder\n",
    "       - Build from list of folders\n",
    "       - Build when the targets and features are in the different files\n",
    "   - Build from pandas DataFrame, and from list of DataFrames\n",
    "   - Splitting data to few categories with parameter coreset_by\n",
    "   - Build from dataset(s) in form of numpy arrays\n",
    "\n",
    "In this example we'll be using the well-known Covertype Dataset (https://archive.ics.uci.edu/ml/datasets/covertype).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_covtype\n",
    "import numpy as np\n",
    "\n",
    "from dataheroes.services import CoresetTreeServiceLG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Covertype dataset as a pandas data frame.\n",
    "# In the output data frame all columns are features beside the last column.\n",
    "# The last column (Cover_Type) is the target\n",
    "df = fetch_covtype(as_frame=True).frame\n",
    "\n",
    "# Split dataframe: df1 = 50%, df2=25%, df3=25%\n",
    "df1, df2 = train_test_split(df, test_size=0.5, random_state=42)\n",
    "df2, df3 = train_test_split(df2, test_size=0.5, random_state=42)\n",
    "\n",
    "# Prepare data directory and set the file names.\n",
    "data1_dir = Path(\"data1_dir\")\n",
    "data2_dir = Path(\"data2_dir\")\n",
    "data1_dir.mkdir(parents=True, exist_ok=True)\n",
    "data2_dir.mkdir(parents=True, exist_ok=True)\n",
    "data1_file_path = data1_dir / \"data1.csv\"\n",
    "data2_file_path = data1_dir / \"data2.csv\"\n",
    "data3_file_path = data2_dir / \"data3.csv\"\n",
    "\n",
    "# Store data as CSV.\n",
    "# After that we have the following structure:\n",
    "#   data1_dir\n",
    "#       data1.csv (~290,000 samples)\n",
    "#       data2.csv (~145,000 samples)\n",
    "#   data2_dir\n",
    "#       data3.csv (~145,000 samples)\n",
    "df1.to_csv(data1_file_path, index=False)\n",
    "df2.to_csv(data2_file_path, index=False)\n",
    "df3.to_csv(data3_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build the tree from file or files\n",
    "Run `build_from_file` on the first file.\n",
    "It will include ~290K sample. Let's use `sample_size` of 10K and `coreset_size` of 2K. Besides csv any format could be used, trough setting `reader_f` and `reader_kwargs` params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell the tree how data is structured.\n",
    "# In this example we have one target column, all other columns are features.\n",
    "data_params = {'target': {'name': 'Cover_Type'}}\n",
    "# Initialize the service and build the tree.\n",
    "# The tree uses the local file system to store its data.\n",
    "# After this step you will have a new directory .dataheroes_cache\n",
    "service_obj = CoresetTreeServiceLG(coreset_size=2_000, data_params=data_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Build the coreset tree with single file\n",
    "Build method returns reference to service_obj, we supress this unnecessary output through `%%capture`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "service_obj.build_from_file(data1_file_path, sample_size=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Build the coreset tree with directory (that contains two files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# For building the tree from the scratch we should initialize new service object\n",
    "service_obj = CoresetTreeServiceLG(coreset_size=2_000, data_params=data_params)\n",
    "service_obj.build_from_file(data1_dir, sample_size=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Build the coreset tree with list of files\n",
    "(Not only lists, but any Iterators could be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "service_obj = CoresetTreeServiceLG(coreset_size=2_000, data_params=data_params)\n",
    "service_obj.build_from_file([data1_file_path, data3_file_path], sample_size=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Build the coreset tree with list of directories (all 3 files should be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "service_obj = CoresetTreeServiceLG(coreset_size=2_000, data_params=data_params)\n",
    "service_obj.build_from_file([data1_dir, data2_dir], sample_size=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build when the targets and features are in the different files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Split target (last column) and features (all another columns)\n",
    "df1_X = df1.iloc[:, :-1]\n",
    "df1_y = df1.iloc[:, -1]\n",
    "# Prepare directory\n",
    "data3_dir = Path(\"data3_dir\")\n",
    "data3_dir.mkdir(parents=True, exist_ok=True)\n",
    "# Store features and targets in two files\n",
    "data1_X_file_path = data3_dir / \"data1_X.csv\"\n",
    "data1_y_file_path = data3_dir / \"data1_y.csv\"\n",
    "df1_X.to_csv(data1_X_file_path, index=False)\n",
    "df1_y.to_csv(data1_y_file_path, index=False)\n",
    "\n",
    "service_obj = CoresetTreeServiceLG(coreset_size=2_000, data_params=data_params)\n",
    "service_obj.build_from_file(data1_X_file_path, target_file_path=data1_y_file_path, sample_size=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build when we coreset_by on the elevation feature.\n",
    "We should have a function that split data to tree nodes on the following way:\n",
    " Elevation < 2400, 2400-2449, 2450-2499, 2500..., 3250-3300, >3300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "def coreset_by_elevation(X):\n",
    "    # list of boundaries [2400, 2450, 2500, ... 3300]\n",
    "    boundaries = [2400 + i * 50 for i in range(19)]\n",
    "    # X[0] - Elevation is first feature in dataset\n",
    "    # We should return index of interval\n",
    "    return np.searchsorted(boundaries, X[0])\n",
    "\n",
    "service_obj = CoresetTreeServiceLG(coreset_size=2_000, data_params=data_params)\n",
    "service_obj.build_from_file(data1_file_path, sample_size=10_000, coreset_by=coreset_by_elevation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build with pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "service_obj = CoresetTreeServiceLG(coreset_size=2_000, data_params=data_params)\n",
    "service_obj.build_from_df(df1, sample_size=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build with list of pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "service_obj = CoresetTreeServiceLG(coreset_size=2_000, data_params=data_params)\n",
    "service_obj.build_from_df([df1, df2], sample_size=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Build with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "service_obj = CoresetTreeServiceLG(coreset_size=2_000, data_params=data_params)\n",
    "# Prepare dataset in form of numpy arrays, features and targets separately\n",
    "X = df1.iloc[:, :-1].to_numpy()\n",
    "y = df1.iloc[:, -1].to_numpy()\n",
    "# Build\n",
    "service_obj.build(X, y, sample_size=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Build with list of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "service_obj = CoresetTreeServiceLG(coreset_size=2_000, data_params=data_params)\n",
    "# Prepare dataset from first dataframe\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "# Same for second dataframe\n",
    "X2 = df2.iloc[:, :-1].to_numpy()\n",
    "y2 = df2.iloc[:, -1].to_numpy()\n",
    "# Build with two datasets\n",
    "service_obj.build([X1,X2], [y1,y2], sample_size=10_000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
