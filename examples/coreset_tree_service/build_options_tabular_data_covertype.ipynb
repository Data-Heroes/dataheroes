{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "In this example we will demonstrate how to:\n",
    "   - Build a Coreset tree from file(s):\n",
    "       - Build from a single file\n",
    "       - Build from a list of files\n",
    "       - Build from all files in a folder\n",
    "       - Build from a list of folders\n",
    "       - Build when the target and features are in the different files\n",
    "   - Build from a pandas DataFrame, and from list of DataFrames\n",
    "   - Build while splitting the data to a few categories with the coreset_by parameter\n",
    "   - Build from dataset(s) in the form of numpy arrays\n",
    "\n",
    "In this example we'll be using the well-known Covertype Dataset (https://archive.ics.uci.edu/ml/datasets/covertype).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_covtype\n",
    "import numpy as np\n",
    "\n",
    "from dataheroes import CoresetTreeServiceLG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Load Covertype dataset as a pandas data frame.\n",
    "# In the output data frame all columns are features beside the last column.\n",
    "# The last column (Cover_Type) is the target\n",
    "df = fetch_covtype(as_frame=True).frame\n",
    "\n",
    "# Split dataframe: df1 = 50%, df2=25%, df3=25%\n",
    "df1, df2 = train_test_split(df, test_size=0.5, random_state=42)\n",
    "df2, df3 = train_test_split(df2, test_size=0.5, random_state=42)\n",
    "\n",
    "# Prepare data directory and set the file names.\n",
    "data1_dir = Path(\"data1_dir\")\n",
    "data2_dir = Path(\"data2_dir\")\n",
    "data1_dir.mkdir(parents=True, exist_ok=True)\n",
    "data2_dir.mkdir(parents=True, exist_ok=True)\n",
    "data1_file_path = data1_dir / \"data1.csv\"\n",
    "data2_file_path = data1_dir / \"data2.csv\"\n",
    "data3_file_path = data2_dir / \"data3.csv\"\n",
    "\n",
    "# Store data as CSV.\n",
    "# After that we will have the following structure:\n",
    "#   data1_dir\n",
    "#       data1.csv (~290,000 samples)\n",
    "#       data2.csv (~145,000 samples)\n",
    "#   data2_dir\n",
    "#       data3.csv (~145,000 samples)\n",
    "df1.to_csv(data1_file_path, index=False)\n",
    "df2.to_csv(data2_file_path, index=False)\n",
    "df3.to_csv(data3_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build the Coreset tree from a file or multiple files\n",
    "Run `build_from_file` on the first file. It will include ~290K sample. \n",
    "\n",
    "Besides the csv format any format could be used, by  setting the `reader_f` and `reader_kwargs` params. \n",
    "\n",
    "We pass `n_classes` and `n_instances` to help the tree calculate an optimal Coreset size. Depending on task type `optimized_for` could be `cleaning` or `training`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Tell the tree how data is structured.\n",
    "# In this example we have one target column, all other columns are features.\n",
    "data_params = {'target': {'name': 'Cover_Type'}}\n",
    "# Initialize the service and build the tree.\n",
    "# The tree uses the local file system to store its data.\n",
    "# After this step you will have a new directory .dataheroes_cache\n",
    "service_obj = CoresetTreeServiceLG(data_params=data_params, \n",
    "                                   optimized_for='training',\n",
    "                                   n_classes=7,\n",
    "                                   n_instances=290_000\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Build the coreset tree with a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataheroes.services.tree_services.CoresetTreeServiceLG at 0x164a99d00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_obj.build_from_file(data1_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Build the coreset tree with a directory (containing two files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataheroes.services.tree_services.CoresetTreeServiceLG at 0x1694f5cd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For building the tree from scratch we should initialize a new service\n",
    "service_obj = CoresetTreeServiceLG(data_params=data_params, \n",
    "                                   optimized_for='cleaning',\n",
    "                                   n_classes=7,\n",
    "                                   n_instances=435_000\n",
    "                                  )\n",
    "service_obj.build_from_file(data1_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Build the coreset tree with a list of files\n",
    "(Not only lists, but any Iterators could be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataheroes.services.tree_services.CoresetTreeServiceLG at 0x169942cd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_obj = CoresetTreeServiceLG(data_params=data_params, \n",
    "                                   optimized_for='cleaning',\n",
    "                                   n_classes=7,\n",
    "                                   n_instances=435_000\n",
    "                                  )\n",
    "service_obj.build_from_file([data1_file_path, data3_file_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Build the coreset tree with a list of directories (all 3 files should be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataheroes.services.tree_services.CoresetTreeServiceLG at 0x16964d6d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_obj = CoresetTreeServiceLG(data_params=data_params, \n",
    "                                   optimized_for='cleaning',\n",
    "                                   n_classes=7,\n",
    "                                   n_instances=580_000\n",
    "                                  )\n",
    "service_obj.build_from_file([data1_dir, data2_dir])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build when the target and features are in the different files.\n",
    "Do a build optimized for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataheroes.services.tree_services.CoresetTreeServiceLG at 0x164b6fa00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split target (last column) and features (all another columns)\n",
    "df1_X = df1.iloc[:, :-1]\n",
    "df1_y = df1.iloc[:, -1]\n",
    "# Prepare directory\n",
    "data3_dir = Path(\"data3_dir\")\n",
    "data3_dir.mkdir(parents=True, exist_ok=True)\n",
    "# Store features and target in two separate files\n",
    "data1_X_file_path = data3_dir / \"data1_X.csv\"\n",
    "data1_y_file_path = data3_dir / \"data1_y.csv\"\n",
    "df1_X.to_csv(data1_X_file_path, index=False)\n",
    "df1_y.to_csv(data1_y_file_path, index=False)\n",
    "\n",
    "service_obj = CoresetTreeServiceLG(data_params=data_params, \n",
    "                                   optimized_for='training',\n",
    "                                   n_classes=7,\n",
    "                                   n_instances=290_000)\n",
    "service_obj.build_from_file(data1_X_file_path, target_file_path=data1_y_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build when we coreset_by the elevation feature.\n",
    "We want to have a function that splits the data to tree nodes in the following way:\n",
    " Elevation < 2400, 2400-2449, 2450-2499, 2500..., 3250-3300, >3300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataheroes.services.tree_services.CoresetTreeServiceLG at 0x1699426a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def coreset_by_elevation(X):\n",
    "    # list of boundaries [2400, 2450, 2500, ... 3300]\n",
    "    boundaries = [2400 + i * 50 for i in range(19)]\n",
    "    # X[0] - Elevation is first feature in dataset\n",
    "    # We should return index of interval\n",
    "    return np.searchsorted(boundaries, X[0])\n",
    "\n",
    "service_obj = CoresetTreeServiceLG(data_params=data_params, \n",
    "                                   optimized_for='cleaning',\n",
    "                                   coreset_size=2_000\n",
    "                                  )\n",
    "service_obj.build_from_file(data1_file_path, coreset_by=coreset_by_elevation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build with a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataheroes.services.tree_services.CoresetTreeServiceLG at 0x169947d90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_obj = CoresetTreeServiceLG(data_params=data_params, \n",
    "                                   optimized_for='cleaning',\n",
    "                                   n_classes=7,\n",
    "                                   n_instances=290_000\n",
    "                                  )\n",
    "service_obj.build_from_df(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build with a list of pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataheroes.services.tree_services.CoresetTreeServiceLG at 0x1694a5370>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_obj = CoresetTreeServiceLG(data_params=data_params, \n",
    "                                   optimized_for='cleaning',\n",
    "                                   n_classes=7,\n",
    "                                   n_instances=435_000\n",
    "                                  )\n",
    "service_obj.build_from_df([df1, df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Build with a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataheroes.services.tree_services.CoresetTreeServiceLG at 0x169d75850>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_obj = CoresetTreeServiceLG(data_params=data_params, \n",
    "                                   optimized_for='cleaning',\n",
    "                                   n_classes=7,\n",
    "                                   n_instances=290_000\n",
    "                                  )\n",
    "# Prepare the dataset in form of numpy arrays, where features and target are separate\n",
    "X = df1.iloc[:, :-1].to_numpy()\n",
    "y = df1.iloc[:, -1].to_numpy()\n",
    "# Build\n",
    "service_obj.build(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Build with a list of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataheroes.services.tree_services.CoresetTreeServiceLG at 0x169d56580>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_obj = CoresetTreeServiceLG(data_params=data_params, \n",
    "                                   optimized_for='cleaning',\n",
    "                                   n_classes=7,\n",
    "                                   n_instances=435_000\n",
    "                                  )\n",
    "# Prepare dataset from first dataframe\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "# Same for second dataframe\n",
    "X2 = df2.iloc[:, :-1].to_numpy()\n",
    "y2 = df2.iloc[:, -1].to_numpy()\n",
    "# Build with two datasets\n",
    "service_obj.build([X1,X2], [y1,y2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Build using the coreset_size and chunk_size directly\n",
    "Instead of passing `n_classes` and `n_instances` in order to help the optimizer calculate the `coreset_size` and `chunk_size`, we can pass these params directly. We will use `chunk_size` of 10K and `coreset_size` of 2K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataheroes.services.tree_services.CoresetTreeServiceLG at 0x1694a50d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_obj = CoresetTreeServiceLG(data_params=data_params, \n",
    "                                   optimized_for='cleaning',\n",
    "                                   chunk_size=10_000,\n",
    "                                   coreset_size=2_000,\n",
    "                                  )\n",
    "service_obj.build_from_file(data1_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
