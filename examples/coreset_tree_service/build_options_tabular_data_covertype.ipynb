{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Options\n",
    "\n",
    "## Purpose\n",
    "In this example we will demonstrate how to:\n",
    "   - Build a Coreset tree from file(s):\n",
    "       - Build from a single file\n",
    "       - Build from a list of files\n",
    "       - Build from all files in a folder\n",
    "       - Build from a list of folders\n",
    "       - Build when the target and features are in the different files\n",
    "   - Build from a pandas DataFrame, and from list of DataFrames\n",
    "   - Build while splitting the data to a few categories with the chunk_by parameter\n",
    "   - Build from dataset(s) in the form of numpy arrays\n",
    "\n",
    "In this example we'll be using the well-known Covertype Dataset (https://archive.ics.uci.edu/ml/datasets/covertype).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T21:49:12.969179Z",
     "start_time": "2024-01-22T21:49:12.939748Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_covtype\n",
    "import numpy as np\n",
    "\n",
    "from dataheroes import CoresetTreeServiceLG\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T21:49:20.925078Z",
     "start_time": "2024-01-22T21:49:12.951685Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Covertype dataset as a pandas data frame.\n",
    "# In the output data frame all columns are features beside the last column.\n",
    "# The last column (Cover_Type) is the target\n",
    "df = fetch_covtype(as_frame=True).frame\n",
    "\n",
    "# Split dataframe: df1 = 50%, df2=25%, df3=25%\n",
    "df1, df2 = train_test_split(df, test_size=0.5, random_state=42)\n",
    "df2, df3 = train_test_split(df2, test_size=0.5, random_state=42)\n",
    "\n",
    "# Prepare data directory and set the file names.\n",
    "data1_dir = Path(\"data1_dir\")\n",
    "data2_dir = Path(\"data2_dir\")\n",
    "data1_dir.mkdir(parents=True, exist_ok=True)\n",
    "data2_dir.mkdir(parents=True, exist_ok=True)\n",
    "data1_file_path = data1_dir / \"data1.csv\"\n",
    "data2_file_path = data1_dir / \"data2.csv\"\n",
    "data3_file_path = data2_dir / \"data3.csv\"\n",
    "\n",
    "# Store data as CSV.\n",
    "# After that we will have the following structure:\n",
    "#   data1_dir\n",
    "#       data1.csv (~290,000 samples)\n",
    "#       data2.csv (~145,000 samples)\n",
    "#   data2_dir\n",
    "#       data3.csv (~145,000 samples)\n",
    "df1.to_csv(data1_file_path, index=False)\n",
    "df2.to_csv(data2_file_path, index=False)\n",
    "df3.to_csv(data3_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build the Coreset tree from a file or multiple files\n",
    "Run `build_from_file` on the first file. It will include ~290K sample. \n",
    "\n",
    "Besides the csv format any format could be used, by  setting the `reader_f` and `reader_kwargs` params. \n",
    "\n",
    "We pass `n_classes` and `n_instances` to help the tree calculate an optimal Coreset size. Depending on task type `optimized_for` could be `cleaning` or `training`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T21:49:20.939758Z",
     "start_time": "2024-01-22T21:49:20.926966Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tell the tree how data is structured.\n",
    "# In this example we have one target column, all other columns are features.\n",
    "data_params = {'target': {'name': 'Cover_Type'}}\n",
    "# Initialize the service and build the tree.\n",
    "# The tree uses the local file system to store its data.\n",
    "# After this step you will have a new directory .dataheroes_cache\n",
    "service_obj = CoresetTreeServiceLG(data_params=data_params, \n",
    "                                   optimized_for='training',\n",
    "                                   n_classes=7,\n",
    "                                   n_instances=290_000\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Build the coreset tree with a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T21:49:23.113133Z",
     "start_time": "2024-01-22T21:49:20.940265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<dataheroes.services.coreset_tree.lg.CoresetTreeServiceLG at 0x2ea6958d0>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_obj.build_from_file(data1_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Build the coreset tree with a directory (containing two files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T21:49:26.037339Z",
     "start_time": "2024-01-22T21:49:23.115854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<dataheroes.services.coreset_tree.lg.CoresetTreeServiceLG at 0x2ea1b2080>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For building the tree from scratch we should initialize a new service\n",
    "service_obj = CoresetTreeServiceLG(data_params=data_params, \n",
    "                                   optimized_for='training',\n",
    "                                   n_classes=7,\n",
    "                                   n_instances=435_000\n",
    "                                  )\n",
    "service_obj.build_from_file(data1_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Build the coreset tree with a list of files\n",
    "(Not only lists, but any Iterators could be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T21:49:28.933982Z",
     "start_time": "2024-01-22T21:49:26.038419Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<dataheroes.services.coreset_tree.lg.CoresetTreeServiceLG at 0x2ea67f490>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_obj = CoresetTreeServiceLG(data_params=data_params, \n",
    "                                   optimized_for='training',\n",
    "                                   n_classes=7,\n",
    "                                   n_instances=435_000\n",
    "                                  )\n",
    "service_obj.build_from_file([data1_file_path, data3_file_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Build the coreset tree with a list of directories (all 3 files should be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T21:49:33.934847Z",
     "start_time": "2024-01-22T21:49:28.935113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<dataheroes.services.coreset_tree.lg.CoresetTreeServiceLG at 0x2ea18df30>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_obj = CoresetTreeServiceLG(data_params=data_params, \n",
    "                                   optimized_for='training',\n",
    "                                   n_classes=7,\n",
    "                                   n_instances=580_000\n",
    "                                  )\n",
    "service_obj.build_from_file([data1_dir, data2_dir])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build when the target and features are in the different files.\n",
    "Do a build optimized for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T21:49:39.790319Z",
     "start_time": "2024-01-22T21:49:33.930208Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<dataheroes.services.coreset_tree.lg.CoresetTreeServiceLG at 0x2ec2bb310>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split target (last column) and features (all another columns)\n",
    "df1_X = df1.iloc[:, :-1]\n",
    "df1_y = df1.iloc[:, -1]\n",
    "# Prepare directory\n",
    "data3_dir = Path(\"data3_dir\")\n",
    "data3_dir.mkdir(parents=True, exist_ok=True)\n",
    "# Store features and target in two separate files\n",
    "data1_X_file_path = data3_dir / \"data1_X.csv\"\n",
    "data1_y_file_path = data3_dir / \"data1_y.csv\"\n",
    "df1_X.to_csv(data1_X_file_path, index=False)\n",
    "df1_y.to_csv(data1_y_file_path, index=False)\n",
    "\n",
    "service_obj = CoresetTreeServiceLG(data_params=data_params, \n",
    "                                   optimized_for='training',\n",
    "                                   n_classes=7,\n",
    "                                   n_instances=290_000)\n",
    "service_obj.build_from_file(data1_X_file_path, target_file_path=data1_y_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build when we chunk_by the elevation feature.\n",
    "We want to have a function that splits the data to tree nodes in the following way:\n",
    " Elevation < 2400, 2400-2449, 2450-2499, 2500..., 3250-3300, >3300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T21:49:46.245190Z",
     "start_time": "2024-01-22T21:49:39.790319Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<dataheroes.services.coreset_tree.lg.CoresetTreeServiceLG at 0x2ea3122f0>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_by_elevation(X):\n",
    "    # list of boundaries [2400, 2450, 2500, ... 3300]\n",
    "    boundaries = [2400 + i * 50 for i in range(19)]\n",
    "    # X[0] - Elevation is first feature in dataset\n",
    "    # We should return index of interval\n",
    "    return np.searchsorted(boundaries, X[0])\n",
    "\n",
    "service_obj = CoresetTreeServiceLG(data_params=data_params, \n",
    "                                   optimized_for='training',\n",
    "                                   coreset_size=2_000,\n",
    "                                   chunk_by=chunk_by_elevation,\n",
    "                                  )\n",
    "service_obj.build_from_file(data1_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build with a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T21:49:48.169228Z",
     "start_time": "2024-01-22T21:49:46.246189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<dataheroes.services.coreset_tree.lg.CoresetTreeServiceLG at 0x2ea321c30>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_obj = CoresetTreeServiceLG(data_params=data_params, \n",
    "                                   optimized_for='training',\n",
    "                                   n_classes=7,\n",
    "                                   n_instances=290_000\n",
    "                                  )\n",
    "service_obj.build_from_df(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build with a list of pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T21:49:50.528683Z",
     "start_time": "2024-01-22T21:49:48.169661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<dataheroes.services.coreset_tree.lg.CoresetTreeServiceLG at 0x2ea3209d0>"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_obj = CoresetTreeServiceLG(data_params=data_params, \n",
    "                                   optimized_for='training',\n",
    "                                   n_classes=7,\n",
    "                                   n_instances=435_000\n",
    "                                  )\n",
    "service_obj.build_from_df([df1, df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build with a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T21:49:52.857682Z",
     "start_time": "2024-01-22T21:49:50.530580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<dataheroes.services.coreset_tree.lg.CoresetTreeServiceLG at 0x2ea3e3040>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_obj = CoresetTreeServiceLG(data_params=data_params, \n",
    "                                   optimized_for='training',\n",
    "                                   n_classes=7,\n",
    "                                   n_instances=290_000\n",
    "                                  )\n",
    "# Prepare the dataset in form of numpy arrays, where features and target are separate\n",
    "X = df1.iloc[:, :-1].to_numpy()\n",
    "y = df1.iloc[:, -1].to_numpy()\n",
    "# Build\n",
    "service_obj.build(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build with a list of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T21:49:56.115444Z",
     "start_time": "2024-01-22T21:49:52.859639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<dataheroes.services.coreset_tree.lg.CoresetTreeServiceLG at 0x2ea4a6d10>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_obj = CoresetTreeServiceLG(data_params=data_params, \n",
    "                                   optimized_for='training',\n",
    "                                   n_classes=7,\n",
    "                                   n_instances=435_000\n",
    "                                  )\n",
    "# Prepare dataset from first dataframe\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "# Same for second dataframe\n",
    "X2 = df2.iloc[:, :-1].to_numpy()\n",
    "y2 = df2.iloc[:, -1].to_numpy()\n",
    "# Build with two datasets\n",
    "service_obj.build([X1,X2], [y1,y2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Build using the coreset_size and chunk_size directly\n",
    "Instead of passing `n_classes` and `n_instances` in order to help the optimizer calculate the `coreset_size` and `chunk_size`, we can pass these params directly. We will use `chunk_size` of 10K and `coreset_size` of 2K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T21:49:59.066945Z",
     "start_time": "2024-01-22T21:49:56.112386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<dataheroes.services.coreset_tree.lg.CoresetTreeServiceLG at 0x2ea4a53c0>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chunk_by will override the chunk_size parameter. (coreset_size is required due to a library error).\n",
    "service_obj = CoresetTreeServiceLG(data_params=data_params,\n",
    "                                   optimized_for='training',\n",
    "                                   chunk_size=10_000,\n",
    "                                   coreset_size=2_000,\n",
    "                                  )\n",
    "service_obj.build_from_file(data1_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
